{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Alibaba Cloud.\n",
    "\n",
    "import copy\n",
    "import re\n",
    "from threading import Thread\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, TextIteratorStreamer\n",
    "\n",
    "DEFAULT_CKPT_PATH = 'Qwen/Qwen2.5-VL-7B-Instruct'\n",
    "\n",
    "# Define arguments manually (instead of argparse)\n",
    "class Args:\n",
    "    checkpoint_path = DEFAULT_CKPT_PATH\n",
    "    cpu_only = False\n",
    "    flash_attn2 = False\n",
    "    share = False\n",
    "    inbrowser = False\n",
    "    server_port = 7860\n",
    "    server_name = \"127.0.0.1\"\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load Model and Processor\n",
    "def _load_model_processor(args):\n",
    "    device_map = 'cpu' if args.cpu_only else 'auto'\n",
    "    \n",
    "    if args.flash_attn2:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            args.checkpoint_path,\n",
    "            torch_dtype=\"auto\",\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            device_map=device_map\n",
    "        )\n",
    "    else:\n",
    "        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(args.checkpoint_path, device_map=device_map)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(args.checkpoint_path)\n",
    "    return model, processor\n",
    "\n",
    "model, processor = _load_model_processor(args)\n",
    "\n",
    "# Define text and image parsing functions\n",
    "def _parse_text(text):\n",
    "    lines = text.split('\\n')\n",
    "    lines = [line for line in lines if line != '']\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if '```' in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = '<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace('`', r'\\`').replace('<', '&lt;').replace('>', '&gt;')\n",
    "                    line = line.replace(' ', '&nbsp;').replace('*', '&ast;').replace('_', '&lowbar;')\n",
    "                    line = line.replace('-', '&#45;').replace('.', '&#46;').replace('!', '&#33;')\n",
    "                    line = line.replace('(', '&#40;').replace(')', '&#41;').replace('$', '&#36;')\n",
    "                lines[i] = '<br>' + line\n",
    "    text = ''.join(lines)\n",
    "    return text\n",
    "\n",
    "def _remove_image_special(text):\n",
    "    return re.sub(r'<box>.*?(</box>|$)', '', text.replace('<ref>', '').replace('</ref>', ''))\n",
    "\n",
    "# Define chatbot processing functions\n",
    "def call_local_model(model, processor, messages):\n",
    "    messages = _transform_messages(messages)\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors='pt')\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    tokenizer = processor.tokenizer\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=20.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    gen_kwargs = {\"max_new_tokens\": 512, \"streamer\": streamer, **inputs}\n",
    "    thread = Thread(target=model.generate, kwargs=gen_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    generated_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        generated_text += new_text\n",
    "        yield generated_text\n",
    "\n",
    "def predict(chatbot, task_history):\n",
    "    chat_query = chatbot[-1][0]\n",
    "    if len(chat_query) == 0:\n",
    "        chatbot.pop()\n",
    "        task_history.pop()\n",
    "        return chatbot\n",
    "\n",
    "    history_cp = copy.deepcopy(task_history)\n",
    "    messages = []\n",
    "    content = []\n",
    "    \n",
    "    for q, a in history_cp:\n",
    "        content.append({\"text\": q})\n",
    "        messages.append({\"role\": \"user\", \"content\": content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": [{\"text\": a}]})\n",
    "        content = []\n",
    "    messages.pop()\n",
    "\n",
    "    for response in call_local_model(model, processor, messages):\n",
    "        chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))\n",
    "        yield chatbot\n",
    "\n",
    "def add_text(history, task_history, text):\n",
    "    history = history if history is not None else []\n",
    "    task_history = task_history if task_history is not None else []\n",
    "    history = history + [(_parse_text(text), None)]\n",
    "    task_history = task_history + [(text, None)]\n",
    "    return history, task_history, \"\"\n",
    "\n",
    "def reset_state(chatbot, task_history):\n",
    "    task_history.clear()\n",
    "    chatbot.clear()\n",
    "    return []\n",
    "\n",
    "# Launch Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"\"\"\\\n",
    "    <p align=\"center\"><img src=\"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\" style=\"height: 80px\"/><p>\"\"\"\n",
    "               )\n",
    "    gr.Markdown(\"\"\"<center><font size=8>Qwen2.5-VL</center>\"\"\")\n",
    "    chatbot = gr.Chatbot(label=\"Qwen2.5-VL\", height=500)\n",
    "    query = gr.Textbox(lines=2, label=\"Input\")\n",
    "    task_history = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"ðŸš€ Submit\")\n",
    "        empty_bin = gr.Button(\"ðŸ§¹ Clear History\")\n",
    "\n",
    "    submit_btn.click(add_text, [chatbot, task_history, query], [chatbot, task_history]).then(\n",
    "        predict, [chatbot, task_history], [chatbot], show_progress=True\n",
    "    )\n",
    "    empty_bin.click(reset_state, [chatbot, task_history], [chatbot], show_progress=True)\n",
    "\n",
    "# Run the Gradio app inside the notebook\n",
    "demo.queue().launch(share=args.share, inbrowser=args.inbrowser, server_port=args.server_port, server_name=args.server_name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
